{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "\n",
    "Bagging is short for Bootstrap Aggregation  \n",
    " \n",
    "    1) Is a process where several models with replacement are built.\n",
    "\n",
    "    2) Average of these models or majority vote are taken to reduce overfitting.\n",
    "\n",
    "\n",
    "Random Forest is a Bagging technique where we construct several trees with replacement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "\n",
    "Is a process in which a set of weak learners create a strong learner.\n",
    "\n",
    "Gradient Boosting - mostly used for regression. \n",
    "\n",
    "Let's look at AdaBoost. \n",
    "\n",
    "We will build a forest of stumps, a stump is a node with leaves. We start with a feature that would give the most information gain or least entropy. \n",
    "\n",
    "<img src=\"sample1.png\" width = 500, height=400>\n",
    "\n",
    "Let's say it is Chest Pain, we built a stump. \n",
    "\n",
    "\n",
    "\n",
    "We find mislcassifed sample(s) and then we have to update their weights.\n",
    "\n",
    "$ error = \\text{sum of the weights of the misclassified samples}$\n",
    "\n",
    "$ \\text{amount of say} = \\frac{1}{2} \\log(\\frac{1 - error}{error}) $ \n",
    "\n",
    "We have to increase the weight(s) of misclassified sample(s) and decrease the weights of the correctly classified sample(s).\n",
    "\n",
    "For increase = old weight * $ e ^{\\text{amount of say}} $\n",
    "\n",
    "For decrease = old weight * $ e ^{ - \\text{amount of say}} $\n",
    "\n",
    "Then normalize the new weights\n",
    "\n",
    "Randomly pick a sample out of the old sample. The chances of the misclassified sample(s) being picked is more. \n",
    "\n",
    "Then we use this distribution for creating a new stump. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the above table and attempt to derive the steps that are outline in the above algorithm:\n",
    "\n",
    "Let us say that the first row is misclassified, then we have to increase its weight \n",
    "\n",
    "error = 0.25\n",
    "\n",
    "amount of say = 1/2 log((1-0.25)/0.25) = 1/2 log(0.75/0.25) = 1/2 * log(3) = 1/2 * 0.47712 = 0.23856\n",
    "\n",
    "For increase = 0.25 * e^(0.23856) = 0.25 * (1.2694) = 0.317354\n",
    "\n",
    "So the weight of the first data point is going to go from 0.25 to 0.3173\n",
    "\n",
    "The weights of the correctly classified data points has to come down\n",
    "\n",
    "For decrease = 0.25 * e^(-0.23856) = 0.25 * (0.78776) = 0.19694\n",
    "\n",
    "So the weights of all the other data points that were correctly classified is going to go from 0.25 to 0.19694\n",
    "\n",
    "data       new weights         normalized weights   \n",
    "\n",
    "1          0.31735         0.3173/0.90817 =  0.34938\n",
    "\n",
    "2          0.19694         0.19694/0.90817 = 0.21685\n",
    "\n",
    "3          0.19694         0.19694/0.90817 = 0.21685\n",
    "\n",
    "4          0.19694         0.19694/0.90817 = 0.21685\n",
    "\n",
    "total      0.90817                           1.0\n",
    "\n",
    "\n",
    "We are not done yet let's add all the new weights . But sum of all weights should be 1, so we divide the new weights by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting\n",
    "\n",
    "https://www.youtube.com/watch?v=3CC4N4z3GJc&t=21s\n",
    "\n",
    "Adaboost \n",
    "\n",
    "https://www.youtube.com/watch?v=LsK-xG1cLYA    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example code of Adaboost with DecisionTree Classifier \n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html\n",
    "\n",
    "In-class activity: Use the heart.csv from the decision_forest folder and apply AdaBoost and Decision Tree with \n",
    "n_estimators = 200.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
